# -*- coding: utf-8 -*-
"""Stonks_CNN_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kRfs7kCeEDVcrzmNkI7rCneHWFFDYzld
"""

!pip install wandb
import wandb
wandb.login()
from wandb.keras import WandbCallback, WandbModelCheckpoint

# from google.colab import drive
# drive.mount('/content/drive/',force_remount=True)

"""# Importing the necessary"""

import numpy as np
import pandas as pd
import os

import matplotlib.pyplot as plt
import seaborn as sns
import time
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB7
import tensorflow.keras.layers as tfl
from sklearn.metrics import confusion_matrix
import gdown

"""# EDA (generating a correlation image)"""

#df_stonks = pd.read_csv('/content/drive/MyDrive/Stonks/1m_open.csv')
file_id = '1lG0YGI-MQQzyu-4_CmanTIc_wnna6JRw'
output = '1m_open.csv'
gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)
df_stonks = pd.read_csv(output)

df_stonks.iloc[200000:210000]

stonk_names = np.array(df_stonks.columns[1:])
# the standard deviation in percentage changes
stonk_std = np.zeros(len(stonk_names))

for i,stonk in enumerate(stonk_names):
  #percentage change in a stonk
  stonk_delta = (np.array(df_stonks[stonk])-np.roll(np.array(df_stonks[stonk]),1))/np.array(df_stonks[stonk])
  #overall std of percentage changes of the stonk
  stonk_std[i] = np.std(stonk_delta)
  #rewrite df_stonks with sigma deviation
  df_stonks[stonk] = stonk_delta/stonk_std[i]

df_stonks.drop([0],inplace = True)

#dataframeidx only refers to
def generate_frame(dataframeidx,stonk_names):
  size = len(stonk_names)
  frame = np.zeros([size,size])
  for i in range(size):
    for j in range(i,size):
      frame[i,j] = dataframeidx[stonk_names[i]]*dataframeidx[stonk_names[j]]
      frame[j,i] = frame[i,j]

  ## Below is only to get into RGB format suitable for EfficientNet negative values will be automatically ignored by RELUs so making everything >=0
  frame = frame- np.min(frame)
  frame = frame/np.max(frame)
  return frame

stonk_names

idx = 50000
image = np.stack([generate_frame(df_stonks.iloc[idx][1:],stonk_names),
                  generate_frame(df_stonks.iloc[idx+1][1:],stonk_names),
                  generate_frame(df_stonks.iloc[idx+2][1:],stonk_names)]).T
plt.imshow(image)

"""# Tensorflow Training Pipeline

### Converting df to a tensor
"""

df_stonk_tensor = tf.convert_to_tensor(df_stonks[stonk_names])

"""### Defining tf functions to generate image of previous 3 min data"""

@tf.function
def generate_frame_tf(stonk_tensor):
  frame = tf.tensordot(stonk_tensor,stonk_tensor,axes = 0)
  ## To normalize between 0 and 1 and to regulate extreme accidental correlations. Can consider others
  frame = (1. + tf.math.tanh(frame))*0.5
  return frame

# @tf.function
# def generate_image_tf(i,stonk_tensor = df_stonk_tensor):
#   image = tf.stack([generate_frame_tf(stonk_tensor[i-1]),generate_frame_tf(stonk_tensor[i-2]),generate_frame_tf(stonk_tensor[i-3])],axis = -1)
#   # 1 refers to AAPL. can chose any other stonk
#   target = tf.math.reduce_max([0.,tf.sign(stonk_tensor[i,1])])
#   return tf.reshape(image,[size,size,3]), tf.reshape(tf.cast(target, tf.float64), [1])

# @tf.function
# def generate_image_tf(i,stonk_tensor = df_stonk_tensor):
#   image = tf.stack([generate_frame_tf(stonk_tensor[i-1]),generate_frame_tf(stonk_tensor[i-2]),generate_frame_tf(stonk_tensor[i-3])],axis = -1)
#   # 1 refers to AAPL. can chose any other stonk
#   target = tf.math.reduce_max([0.,tf.sign(stonk_tensor[i,1])])
#   return tf.reshape(image,[size,size,3]), tf.cast(target, tf.float64)

@tf.function
def generate_image_tf(i,stonk_tensor = df_stonk_tensor):
  image = tf.stack([generate_frame_tf(stonk_tensor[i-1]),generate_frame_tf(stonk_tensor[i-2]),generate_frame_tf(stonk_tensor[i-3])],axis = -1)

  ##Normalizing the image
  image = image - tf.math.reduce_min(image)
  image = (image/tf.math.reduce_max(image))*255
  # 1 refers to AAPL. can chose any other stonk
  target = tf.math.reduce_max([0.,tf.sign(stonk_tensor[i,1])])
  return tf.reshape(image,[size,size,3]), (tf.cast(target, tf.float64), stonk_tensor[i,1])

batch_size = 32
train = tf.data.Dataset.from_tensor_slices((range(4,100000)))
val = tf.data.Dataset.from_tensor_slices((range(100000,110000)))


train = train.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
train = train.batch(batch_size)
#train_unet = train_unet.prefetch(tf.data.AUTOTUNE)

val = val.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
val = val.batch(batch_size)

for el in val.take(0):
  print(el)
  plt.imshow((el[0][3]).numpy())

"""### EfficientNet model"""

preprocess_input = tf.keras.applications.efficientnet.preprocess_input

batch_size = 32

size = 126

IMG_SIZE = (size,size)


def G2_classifier(image_shape=IMG_SIZE):
    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model
    Arguments:
        image_shape -- Image width and height
        data_augmentation -- data augmentation function
    Returns:
    Returns:
        tf.keras.model
    '''


    input_shape = image_shape + (3,)

    ### START CODE HERE

    base_model = EfficientNetB7(input_shape=input_shape,
                                                   include_top=False, # <== Important!!!!
                                                   weights='imagenet') # From imageNet

    # Freeze the base model by making it non trainable
    base_model.trainable = False

    # create theinput layer (Same as the imageNetv2 input size)
    inputs = tf.keras.Input(shape=input_shape)


    # data preprocessing using the same weights the model was trained on
    x = preprocess_input(inputs)

    # set training to False to avoid keeping track of statistics in the batch norm layer
    x = base_model(x, training=False)

    # Add the new Binary classification layers
    # use global avg pooling to summarize the info in each channel
    x = tfl.GlobalAveragePooling2D()(x)
    #include dropout with probability of 0.2 to avoid overfitting
    x = tfl.Dropout(0.2)(x)

    # create a prediction layer with one neuron (as a classifier only needs one)
    prediction_layer = tfl.Dense(1,activation = 'sigmoid')
    #prediction_layer = tfl.Dense(1,activation = 'linear') #from logits in fit

    ### END CODE HERE

    pred = prediction_layer(x)
    outputs = [pred, pred]
    model = tf.keras.Model(inputs, outputs)

    return model

"""### Training tfrecords pipeline"""

batch_size = 32
train = tf.data.Dataset.from_tensor_slices((range(4,100000)))
val = tf.data.Dataset.from_tensor_slices((range(100000,110000)))


train = train.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
train = train.batch(batch_size)
#train_unet = train_unet.prefetch(tf.data.AUTOTUNE)

val = val.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
val = val.batch(batch_size)

train.take(1)

stonk_classifier = G2_classifier(image_shape=(size,size))
stonk_classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              loss=[tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.MeanAbsoluteError()],
              loss_weights = [1, 0],
              metrics=['AUC','accuracy'])

# stonk_classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#               loss=tf.keras.losses.BinaryCrossentropy(),
#               metrics=['AUC','accuracy'])

# # Trained total for 8 epochs
stonk_classifier.fit(x = train,validation_data=val,epochs = 5)

# predict = stonk_classifier.predict(val)

# base_model = stonk_classifier.layers[1]
# base_model.trainable = True
# print("Number of layers in the base model: ", len(base_model.layers))
# fine_tune_at = 700

# ### START CODE HERE

# # Freeze all the layers before the `fine_tune_at` layer
# for layer in base_model.layers[:fine_tune_at]:
#     layer.trainable = False

# # Define a BinaryCrossentropy loss function. Use from_logits=True
# #loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)
# loss_function=tf.keras.losses.BinaryCrossentropy()
# # Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.1*0.001)
# # Use accuracy as evaluation metric
# metrics=['AUC']

# ### END CODE HERE

# stonk_classifier.compile(loss=loss_function,
#               optimizer = optimizer,
#               metrics=metrics)

# stonk_classifier.compile(loss=loss_function,
#               optimizer = optimizer,
#               metrics=['AUC','accuracy'])

def pnl_curve_tensorflow(predictions, price_changes):
    # Ensure inputs are tensors
    predictions = tf.squeeze(tf.convert_to_tensor(predictions, dtype=tf.float32))
    #1 refers to
    price_changes = tf.convert_to_tensor(price_changes, dtype=tf.float32)


    # Calculate daily PnL
    daily_pnl_ob = predictions * (price_changes)
    daily_pnl_bs = ((predictions-0.5)*2) * price_changes
    #print(daily_pnl)

    # Calculate cumulative PnL
    cum_pnl_ob = tf.math.cumprod(daily_pnl_ob+1)
    cum_pnl_bs = tf.math.cumprod(daily_pnl_bs+1)

    return daily_pnl_ob,cum_pnl_ob,daily_pnl_bs,cum_pnl_bs

def train_classifier(config: dict,
          callbacks: list,
          verbose: int=0):
    """
    Utility function to train the model.


    Arguments:
        config (dict): Dictionary of hyperparameters.
        callbacks (list): List of callbacks passed to `model.fit`.
        verbose (int): 0 for silent and 1 for progress bar.
    """


    # Initalize model
    tf.keras.backend.clear_session()
    #model = unet_model_2(input_size=(128,128,1),n_filters=config.nfilters)

    batch_size = config.batch_size
    rtraini = config.itrain
    rtrainf = (config.itrain)+(config.ntrain)
    train = tf.data.Dataset.from_tensor_slices((range(rtraini,rtrainf)))
    val = tf.data.Dataset.from_tensor_slices((range(rtrainf, rtrainf + (config.ntrain)//10)))


    train = train.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
    train = train.batch(batch_size)
    if config.shuffle:
      train = train.shuffle(buffer_size= 100)
    #train_unet = train_unet.prefetch(tf.data.AUTOTUNE)

    val = val.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
    val = val.batch(batch_size)

    #stonk_classifier = tf.keras.models.load_model('/content/drive/MyDrive/Stonks/models/stonk_classifier_base.h5')#G2_classifier(image_shape=(size,size))
    stonk_classifier = G2_classifier(image_shape=(size,size))
    if config.train_all:

      base_model = stonk_classifier.layers[1]
      base_model.trainable = True

      fine_tune_at = 0
      for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
      config.init_learning_rate = 0.1*config.init_learning_rate

    stonk_classifier.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate),
              #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              loss=[tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.MeanAbsoluteError()],
              loss_weights = [1, 0],
              metrics=config.metrics)



    # Train the model
    _ = stonk_classifier.fit(train,
                  epochs= config.epochs,
                  validation_data=val,
                  callbacks=callbacks,
                  verbose=verbose)


    return stonk_classifier

configs = dict(
    batch_size = 32,
    model_name = 'effnetb7',
    epochs = 30,
    init_learning_rate = 0.3*0.0001,
    itrain = 4,
    ntrain = 100000,
    shuffle=True,
    train_all = False,
    loss_fn = tf.keras.losses.BinaryCrossentropy(),
    metrics = ['AUC','accuracy','Precision']

)
run = wandb.init(project='stonks_effnet_nsize', config=configs, job_type='train',settings=wandb.Settings(init_timeout=300))
config = wandb.config


# Define WandbCallback for experiment tracking
wandb_callback = WandbCallback(monitor='val_accuracy',
                               log_weights=True,
                               log_evaluation=True,
                               log_batch_frequency = 100,
                               save_best_only = True,
                               validation_steps=10)

save_path_all = '/content/drive/MyDrive/Stonks/models/stonk_classifier_all_'+str(config.itrain)+'_'+str(config.ntrain)+'.h5'
save_path_base = '/content/drive/MyDrive/Stonks/models/stonk_classifier_base_'+str(config.itrain)+'_'+str(config.ntrain)+'.h5'
# callbacks
if config.train_all:
  callbacks = [wandb_callback, WandbModelCheckpoint(filepath = save_path_all , save_best_only=True)]
else:
  callbacks = [wandb_callback, WandbModelCheckpoint(filepath = save_path_base , save_best_only=True)]

# Train
stonk_classifier = train_classifier(config, callbacks=callbacks, verbose=1)

#wandb.log('train/train_loss',train_loss)
# Evaluate the trained model


rtrainf = (config.itrain)+(config.ntrain)
val_size = (config.ntrain)//10
val = tf.data.Dataset.from_tensor_slices((range(rtrainf, rtrainf + val_size)))
val = val.map(generate_image_tf, num_parallel_calls=1)## Setting this to 1. AUTOTUNE freezing computation
val = val.batch(batch_size)


#loss, auc, accuracy, precision = stonk_classifier.evaluate(val)
results = stonk_classifier.evaluate(val)
loss = results[0]
auc = results[3]
accuracy = results[4]
precision = results[5]
y_pred = stonk_classifier.predict(val)[0]
y = np.concatenate([y[0] for x, y in val], axis=0)
int_steps = 100
step = (y_pred.max()-y_pred.min())/int_steps
accuracy_list = np.zeros(int_steps)
baseline_acc = np.max([y.sum()/(val_size),1 - (y.sum()/(val_size))])
for t in range(int_steps):
  y_pred_binary = (y_pred > (y_pred.min() + step*t))
  c = confusion_matrix(y,y_pred_binary)/(val_size)
  accuracy_list[t] = c[0,0]+c[1,1]
accuracy_list = accuracy_list-baseline_acc
max_accuracy_diff = np.amax(accuracy_list)
y_pred_binary = (y_pred > (y_pred.min() + step*50))
price_changes = []
for el in val.take(313):
  price_changes.append((el[1][1]).numpy())

price_changes = np.concatenate(np.array(price_changes))
ans = pnl_curve_tensorflow(y_pred_binary,price_changes*stonk_std[1])

fig, axs = plt.subplots(2)
axs[0].plot(accuracy_list,color  = 'b')
#ax.plot(accuracy_all, color = 'g')
axs[0].set_ylim(0.0,0.05)
axs[1].plot(ans[1].numpy())
axs[1].plot(ans[3].numpy())
wandb.log({'auc': auc,'accuracy': accuracy, 'acc_list': accuracy_list,'max_diff':max_accuracy_diff})


# Close the W&B run.
wandb.finish()

y_pred_binary = (y_pred >= (y_pred.min() + step*0))
price_changes = []
for el in val.take(313):
  price_changes.append((el[1][1]).numpy())

price_changes = np.concatenate(np.array(price_changes))
print(price_changes[-1] - price_changes[0])

ans = pnl_curve_tensorflow(y_pred_binary,price_changes*stonk_std[1])

fig, axs = plt.subplots(2)
axs[0].plot(accuracy_list,color  = 'b')
#ax.plot(accuracy_all, color = 'g')
axs[0].set_ylim(0.0,0.05)
axs[1].plot(ans[1].numpy())
axs[1].plot(ans[3].numpy())
print(ans[1])

results = stonk_classifier.evaluate(val)
loss = results[0]
auc = results[3]
accuracy = results[4]
precision = results[5]
y_pred = stonk_classifier.predict(val)[0]
y = np.concatenate([y[0] for x, y in val], axis=0)
int_steps = 100
step = (y_pred.max()-y_pred.min())/int_steps
accuracy_list = np.zeros(int_steps)
baseline_acc = np.max([y.sum()/(val_size),1 - (y.sum()/(val_size))])
for t in range(int_steps):
  y_pred_binary = (y_pred > (y_pred.min() + step*t))
  c = confusion_matrix(y,y_pred_binary)/(val_size)
  accuracy_list[t] = c[0,0]+c[1,1]
accuracy_list = accuracy_list-baseline_acc
max_accuracy_diff = np.amax(accuracy_list)
fig, ax = plt.subplots()
ax.plot(accuracy_list,color  = 'b')
#ax.plot(accuracy_all, color = 'g')
ax.set_ylim(0.0,0.05)

# wandb.log({'auc': auc,'accuracy': accuracy, 'acc_list': accuracy_list,'max_diff':max_accuracy_diff})
# # Close the W&B run.
# wandb.finish()

sns.distplot(y_pred)



y_pred_binary = (y_pred > (y_pred.min() + step*50))

y_pred = stonk_classifier.predict(val)[0]
y = np.concatenate([y[0] for x, y in val], axis=0)
int_steps = 100
step = (y_pred.max()-y_pred.min())/int_steps
y_pred_binary = (y_pred > (y_pred.min() + step*50))

price_changes = []
for el in val.take(313):
  price_changes.append((el[1][1]).numpy())

price_changes = np.concatenate(np.array(price_changes))

def pnl_curve_tensorflow(predictions, price_changes):
    # Ensure inputs are tensors
    predictions = tf.squeeze(tf.convert_to_tensor(predictions, dtype=tf.float32))
    #1 refers to
    price_changes = tf.convert_to_tensor(price_changes, dtype=tf.float32)


    # Calculate daily PnL
    daily_pnl_ob = predictions * (price_changes)
    daily_pnl_bs = ((predictions-0.5)*2) * price_changes
    #print(daily_pnl)

    # Calculate cumulative PnL
    cum_pnl_ob = tf.math.cumprod(daily_pnl_ob+1)
    cum_pnl_bs = tf.math.cumprod(daily_pnl_bs+1)

    return daily_pnl_ob,cum_pnl_ob,daily_pnl_bs,cum_pnl_bs

ans = pnl_curve_tensorflow(y_pred_binary,price_changes*stonk_std[1])

for el in val.take(1):
  print(el)

plt.plot(ans[1].numpy())
plt.plot(ans[3].numpy())

ans[1]

results = stonk_classifier.evaluate(val)
loss = results[0]
auc = results[3]
accuracy = results[4]
precision = results[5]
y_pred = stonk_classifier.predict(val)[0]
y = np.concatenate([y[0] for x, y in val], axis=0)
int_steps = 100
step = (y_pred.max()-y_pred.min())/int_steps
accuracy_list = np.zeros(int_steps)
baseline_acc = np.max([y.sum()/(val_size),1 - (y.sum()/(val_size))])
for t in range(int_steps):
  y_pred_binary = (y_pred > (y_pred.min() + step*t))
  c = confusion_matrix(y,y_pred_binary)/(val_size)
  accuracy_list[t] = c[0,0]+c[1,1]
accuracy_list = accuracy_list-baseline_acc
max_accuracy_diff = np.amax(accuracy_list)
fig, ax = plt.subplots()
ax.plot(accuracy_list,color  = 'b')
#ax.plot(accuracy_all, color = 'g')
ax.set_ylim(0.0,0.05)
wandb.log({'auc': auc,'accuracy': accuracy, 'acc_list': accuracy_list,'max_diff':max_accuracy_diff})


# Close the W&B run.
wandb.finish()

stonk_classifier.evaluate(val)

stonk_classifier = tf.keras.models.load_model('/content/drive/MyDrive/Stonks/models/stonk_classifier_base_100000.h5')
#stonk_classifier_all = tf.keras.models.load_model('/content/drive/MyDrive/Stonks/models/stonk_classifier_all.h5')

y_pred = stonk_classifier.predict(val)
#y_pred_all = stonk_classifier_all.predict(val)

sns.distplot(y_pred[:,0])
#sns.distplot(y_pred_all[:,0])

y = np.concatenate([y for x, y in val], axis=0)

int_steps = 100
step = (y_pred.max()-y_pred.min())/int_steps
#step_all = (y_pred_all.max()-y_pred_all.min())/int_steps
accuracy = np.zeros(int_steps)
#accuracy_all = np.zeros(int_steps)
baseline_acc = np.max([y.sum()/((config.ntrain)//10),1 - (y.sum()/((config.ntrain)//10))])
for t in range(int_steps):
  y_pred_binary = (y_pred > (y_pred.min() + step*t))
  c = confusion_matrix(y,y_pred_binary)/((config.ntrain)//10)
  accuracy[t] = c[0,0]+c[1,1]
accuracy = accuracy - baseline_acc
  # y_pred_all_binary = (y_pred_all > (y_pred_all.min() + step_all*t))
  # c_all = confusion_matrix(y,y_pred_all_binary)/(10000)
  # accuracy_all[t] = c_all[0,0]+c_all[1,1]

fig, ax = plt.subplots()
ax.plot(accuracy,color  = 'b')
#ax.plot(accuracy_all, color = 'g')
ax.set_ylim(0.0,0.05)

accuracy

accuracy = accuracy - baseline_acc

accuracy

"""**bold text**# To Do List

1.   Code in returns (and sharpe ratio) instead of just accuracy (use separate val dataset)
2.   Try smaller datasets
3.   Backtesting framework
4.   Monte - Carlo sampling (to validate the strategy)
5.   Try other datasets
6.   Change the initialization for training from scratch ('He Normal')
7.   Choose a baseline (SPY)



"""

